---
title: CosmosDB を使用した小売業における画像検索の入門
author: scseely
ms.author: scseely
ms.date: 11/20/2019
ms.topic: article
ms.service: industry
description: この記事では、オンプレミスから Azure に e コマース インフラストラクチャを移行する各フェーズについて説明します。
ms.openlocfilehash: b43ea305e11ac32da58e4d0521d79f90d5c23d85
ms.sourcegitcommit: 3b175d73a82160c4cacec1ce00c6d804a93c765d
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 02/06/2020
ms.locfileid: "77053149"
---
# <a name="visual-search-overview"></a><span data-ttu-id="76773-103">画像検索の概要</span><span class="sxs-lookup"><span data-stu-id="76773-103">Visual Search Overview</span></span>

## <a name="executive-summary"></a><span data-ttu-id="76773-104">概要</span><span class="sxs-lookup"><span data-stu-id="76773-104">Executive Summary</span></span>

<span data-ttu-id="76773-105">人工知能により、今日の小売業が変革される可能性があります。</span><span class="sxs-lookup"><span data-stu-id="76773-105">Artificial Intelligence offers the potential to transform retailing as we know it today.</span></span> <span data-ttu-id="76773-106">小売業者は AI に支えられたカスタマー エクスペリエンス アーキテクチャを開発するようになると考えてよいでしょう。</span><span class="sxs-lookup"><span data-stu-id="76773-106">It is reasonable to believe that retailers will develop a customer experience architecture supported by AI.</span></span> <span data-ttu-id="76773-107">AI で強化されたプラットフォームでは、ハイパー パーソナル化による収益の増加が期待されます。</span><span class="sxs-lookup"><span data-stu-id="76773-107">Some expectations are that a platform enhanced with AI will provide a revenue bump due to hyper personalization.</span></span> <span data-ttu-id="76773-108">デジタル コマースは、顧客の期待、嗜好、行動を高め続けています。</span><span class="sxs-lookup"><span data-stu-id="76773-108">Digital commerce continues to heighten customer expectations, preferences and behavior.</span></span> <span data-ttu-id="76773-109">リアルタイム エンゲージメント、関連性のあるレコメンデーション、ハイパー パーソナル化などのニーズは、ボタンのクリックによるスピードと利便性の向上を促しています。</span><span class="sxs-lookup"><span data-stu-id="76773-109">Demands such as real-time engagement, relevant recommendations and hyper-personalization are driving speed and convenience at a click of a button.</span></span> <span data-ttu-id="76773-110">自然音声や視覚などを通じてアプリケーションのインテリジェンスを実現することで、小売業において、顧客の従来の買物の方法を覆し、価値を増大させる改善が可能になります。</span><span class="sxs-lookup"><span data-stu-id="76773-110">Enabling intelligence in applications through natural speech, vision, etc. enables improvements in retail that will increase value while disrupting how customers shop.</span></span>

<span data-ttu-id="76773-111">このドキュメントでは、**画像検索**の AI の概念について説明し、その実装に関する重要な考慮事項を示します。</span><span class="sxs-lookup"><span data-stu-id="76773-111">This document focuses on the AI  concept of **visual search** and offers a few key considerations on its implementation.</span></span> <span data-ttu-id="76773-112">ワークフローの例を示し、各段階を関連する Azure テクノロジにマップします。</span><span class="sxs-lookup"><span data-stu-id="76773-112">It provides a workflow example and maps its stages to the relevant Azure technologies.</span></span> <span data-ttu-id="76773-113">概念は、顧客がエクスペリエンスの意図に応じて、モバイル デバイスで撮影した画像やインターネット上にある画像を利用して、関連する項目や類似する項目の検索を実行できることに基づいています。</span><span class="sxs-lookup"><span data-stu-id="76773-113">The concept is based on a customer being able to leverage an image taken with their mobile device or located on the internet to conduct a search of relevant and/or like items, depending upon the intention of the experience.</span></span> <span data-ttu-id="76773-114">そのため、画像検索では、テキスト入力ではなく、複数のメタデータ ポイントを含む画像を使用することで速度が向上し、適切な項目をすばやく見つけ出すことができます。</span><span class="sxs-lookup"><span data-stu-id="76773-114">Thus, visual search improves speed from texted entry to an image with multiple meta-data points to quickly surface applicable items available.</span></span>

## <a name="visual-search-engines"></a><span data-ttu-id="76773-115">画像検索エンジン</span><span class="sxs-lookup"><span data-stu-id="76773-115">Visual Search Engines</span></span>

<span data-ttu-id="76773-116">画像検索エンジンは、画像を入力として使用して情報を取得しますが、それだけではなく、多くの場合、出力としても使用します。</span><span class="sxs-lookup"><span data-stu-id="76773-116">Visual search engines retrieve information using images as input and often—but not exclusively—as output too.</span></span>

<span data-ttu-id="76773-117">画像検索エンジンは小売業界で普及が進んでいますが、これには次のような十分な理由があります。</span><span class="sxs-lookup"><span data-stu-id="76773-117">Engines are becoming more and more common in the retail industry, and for very good reasons:</span></span>

- <span data-ttu-id="76773-118">2017 年に発行された [Emarketer](https://www.emarketer.com/Report/Visual-Commerce-2017-How-Image-Recognition-Augmentation-Changing-Retail/2002059) のレポートによると、インターネット ユーザーの約 75% が、購入前に製品の写真やビデオを検索しています。</span><span class="sxs-lookup"><span data-stu-id="76773-118">Around 75% of internet users search for pictures or videos of a product before making a purchase, according to an [Emarketer](https://www.emarketer.com/Report/Visual-Commerce-2017-How-Image-Recognition-Augmentation-Changing-Retail/2002059) report published in 2017.</span></span>
- <span data-ttu-id="76773-119">[Slyce](https://slyce.it/wp-content/uploads/2015/11/Visual_Search_Technology_and_Market.pdf) (画像検索会社) の 2015 年のレポートによると、コンシューマーの 74% が、テキスト検索は非効率的であると思っています。</span><span class="sxs-lookup"><span data-stu-id="76773-119">74% of consumers also find text searches inefficient, according to [Slyce](https://slyce.it/wp-content/uploads/2015/11/Visual_Search_Technology_and_Market.pdf) (a visual search company) 2015 report.</span></span>

<span data-ttu-id="76773-120">そのため、[Markets &amp; Markets](https://www.marketsandmarkets.com/PressReleases/image-recognition.asp) の調査によると、画像認識市場は 2019 年までに 250 億ドル以上の規模になると予想されます。</span><span class="sxs-lookup"><span data-stu-id="76773-120">Therefore, the image recognition market will be worth more than $25 billion by 2019, according to research by [Markets &amp; Markets](https://www.marketsandmarkets.com/PressReleases/image-recognition.asp).</span></span>

<span data-ttu-id="76773-121">このテクノロジは、主要 e コマース ブランドで既に採用されています。これらのブランドは、このテクノロジの開発にも大きく貢献しています。</span><span class="sxs-lookup"><span data-stu-id="76773-121">The technology has already taken hold with major e-commerce brands, who have also contributed significantly to its development.</span></span> <span data-ttu-id="76773-122">最も有名と思われる早期導入者は次のとおりです。</span><span class="sxs-lookup"><span data-stu-id="76773-122">The most prominent early adopters are probably:</span></span>

- <span data-ttu-id="76773-123">アプリで Image Search および "Find It on eBay" ツールを提供する eBay (現時点ではモバイル エクスペリエンスのみ)。</span><span class="sxs-lookup"><span data-stu-id="76773-123">eBay with their Image Search and "Find It on eBay" tools in their app (this is currently only a mobile experience).</span></span>
- <span data-ttu-id="76773-124">画像検索ツールのレンズ (Lens) を提供する Pinterest。</span><span class="sxs-lookup"><span data-stu-id="76773-124">Pinterest with their Lens visual discovery tool.</span></span>
- <span data-ttu-id="76773-125">Bing Visual Search を提供する Microsoft。</span><span class="sxs-lookup"><span data-stu-id="76773-125">Microsoft with Bing Visual Search.</span></span>

## <a name="adopt-and-adapt"></a><span data-ttu-id="76773-126">採用と適応</span><span class="sxs-lookup"><span data-stu-id="76773-126">Adopt and Adapt</span></span>

<span data-ttu-id="76773-127">画像検索から利益を得るために膨大な処理能力は不要です。</span><span class="sxs-lookup"><span data-stu-id="76773-127">Fortunately, you don't need vast amounts of computing power to profit from visual search.</span></span> <span data-ttu-id="76773-128">画像カタログを所有する企業は、Azure サービスに組み込まれた Microsoft の AI の専門知識を活用できます。</span><span class="sxs-lookup"><span data-stu-id="76773-128">Any business with an image catalog can take advantage of Microsoft's AI expertise built into its Azure services.</span></span>

<span data-ttu-id="76773-129">[Bing Visual Search](https://azure.microsoft.com/services/cognitive-services/bing-visual-search/?WT.mc_id=vsearchgio-article-gmarchet) API を使用すると、画像からコンテキスト情報を抽出し、たとえば、家財道具、ファッション、数種類の製品などを識別できます。</span><span class="sxs-lookup"><span data-stu-id="76773-129">[Bing Visual Search](https://azure.microsoft.com/services/cognitive-services/bing-visual-search/?WT.mc_id=vsearchgio-article-gmarchet) API provides a way to extract context information from images, identifying—for instance—home furnishings, fashion, several kinds of products, etc.</span></span>

<span data-ttu-id="76773-130">この API では、独自のカタログ、関連するショッピング ソースの製品、関連検索から、見た目が似ている画像を返すこともできます。</span><span class="sxs-lookup"><span data-stu-id="76773-130">It will also return visually similar images out of its own catalog, products with relative shopping sources, related searches.</span></span> <span data-ttu-id="76773-131">興味深いものの、自社がこれらのソースの 1 つでない場合、これはあまり役に立ちません。</span><span class="sxs-lookup"><span data-stu-id="76773-131">While interesting, this will be of limited use if your company is not one of those sources.</span></span>

<span data-ttu-id="76773-132">Bing には次の機能も用意されています。</span><span class="sxs-lookup"><span data-stu-id="76773-132">Bing will also provide:</span></span>

- <span data-ttu-id="76773-133">画像に含まれるオブジェクトや概念を探索できるタグ。</span><span class="sxs-lookup"><span data-stu-id="76773-133">Tags that allow you to explore objects or concepts found in the image.</span></span>
- <span data-ttu-id="76773-134">画像内の関心領域 (衣類、家具など) を示す境界ボックス。</span><span class="sxs-lookup"><span data-stu-id="76773-134">Bounding boxes for regions of interest in the image (e.g. clothing, furniture items).</span></span>

<span data-ttu-id="76773-135">その情報を取得して、検索空間を会社の製品カタログに限定し、関心領域の対象となるカテゴリのオブジェクトなどに限定することで、時間を大幅に短縮できます。</span><span class="sxs-lookup"><span data-stu-id="76773-135">You can take that information to reduce the search space (and time) into a company's product catalog significantly, restricting it to objects like those in the region and category of interest.</span></span>

## <a name="implement-your-own"></a><span data-ttu-id="76773-136">独自の画像検索の実装</span><span class="sxs-lookup"><span data-stu-id="76773-136">Implement Your Own</span></span>

<span data-ttu-id="76773-137">画像検索を実装するときに考慮すべき重要な要素がいくつかあります。</span><span class="sxs-lookup"><span data-stu-id="76773-137">There are a few key components to consider when implementing visual search:</span></span>

- <span data-ttu-id="76773-138">画像の取り込みとフィルター処理</span><span class="sxs-lookup"><span data-stu-id="76773-138">Ingesting and filtering images</span></span>
- <span data-ttu-id="76773-139">保存と取得の手法</span><span class="sxs-lookup"><span data-stu-id="76773-139">Storage and retrieval techniques</span></span>
- <span data-ttu-id="76773-140">特徴付け、エンコード、または "ハッシュ"</span><span class="sxs-lookup"><span data-stu-id="76773-140">Featurization, encoding or "hashing"</span></span>
- <span data-ttu-id="76773-141">類似性測度または距離とランク付け</span><span class="sxs-lookup"><span data-stu-id="76773-141">Similarity measures or distances and ranking</span></span>

 ![](./assets/visual-search-use-case-overview/visual-search-pipeline.png)

<span data-ttu-id="76773-142">*図 1: 画像検索パイプラインの例*</span><span class="sxs-lookup"><span data-stu-id="76773-142">*Figure 1: Example of Visual Search Pipeline*</span></span>

### <a name="sourcing-the-pictures"></a><span data-ttu-id="76773-143">画像の調達</span><span class="sxs-lookup"><span data-stu-id="76773-143">Sourcing the Pictures</span></span>

<span data-ttu-id="76773-144">画像カタログを所有していない場合、Fashion [MNIST](https://www.kaggle.com/zalando-research/fashionmnist) や Deep [Fashion](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) などの公開されているデータセットでアルゴリズムをトレーニングすることが必要な場合があります。</span><span class="sxs-lookup"><span data-stu-id="76773-144">If you do not own a picture catalog, you may need to train the algorithms on openly available data sets, such as fashion [MNIST](https://www.kaggle.com/zalando-research/fashionmnist), deep [fashion](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) and similar.</span></span> <span data-ttu-id="76773-145">これらのデータセットには複数の製品カテゴリが含まれており、画像分類および検索アルゴリズムのベンチマークに一般に使用されます。</span><span class="sxs-lookup"><span data-stu-id="76773-145">They contain several categories of products and are commonly used to benchmark image categorization and search algorithms.</span></span>

 ![](./assets/visual-search-use-case-overview/deep-fashion-dataset.png)

<span data-ttu-id="76773-146">*図 2: Deep Fashion データセットの例*</span><span class="sxs-lookup"><span data-stu-id="76773-146">*Figure 2: An Example from The Deep Fashion Dataset*</span></span>

### <a name="filtering-the-images"></a><span data-ttu-id="76773-147">画像のフィルター処理</span><span class="sxs-lookup"><span data-stu-id="76773-147">Filtering the Images</span></span>

<span data-ttu-id="76773-148">前述のようなベンチマーク データセットのほとんどは、既に前処理されています。</span><span class="sxs-lookup"><span data-stu-id="76773-148">Most benchmark datasets - such as those mentioned before - have already been pre-processed.</span></span>

<span data-ttu-id="76773-149">独自のデータセットを構築する場合、少なくとも、画像をすべて同じサイズにすることが望まれます。ほとんどの場合、サイズはモデルのトレーニングの対象となった入力によって決まります。</span><span class="sxs-lookup"><span data-stu-id="76773-149">If you are building your own, at a minimum you will want the images to all have the same size, mostly dictated by the input that your model is trained for.</span></span>

<span data-ttu-id="76773-150">多くの場合、画像の明度を正規化することも推奨されます。</span><span class="sxs-lookup"><span data-stu-id="76773-150">In many cases, it is best also to normalize the luminosity of the images.</span></span> <span data-ttu-id="76773-151">検索の詳細レベルによっては、色も冗長情報になる可能性があるため、黒と白に減らすと処理時間が短縮されます。</span><span class="sxs-lookup"><span data-stu-id="76773-151">Depending on the detail level of your search, color may also be redundant information, so reducing to black and white will help with processing times.</span></span>

<span data-ttu-id="76773-152">最後に、画像データセットは、それが表すさまざまなクラス間でバランスを取る必要があります。</span><span class="sxs-lookup"><span data-stu-id="76773-152">Last but not least, the image dataset should be balanced across the different classes it represents.</span></span>

### <a name="image-database"></a><span data-ttu-id="76773-153">画像データベース</span><span class="sxs-lookup"><span data-stu-id="76773-153">Image Database</span></span>

<span data-ttu-id="76773-154">データ レイヤーは、アーキテクチャの特に繊細なコンポーネントです。</span><span class="sxs-lookup"><span data-stu-id="76773-154">The data layer is a particularly delicate component of your architecture.</span></span> <span data-ttu-id="76773-155">データ レイヤーには以下が含まれます。</span><span class="sxs-lookup"><span data-stu-id="76773-155">It will contain:</span></span>

- <span data-ttu-id="76773-156">イメージ</span><span class="sxs-lookup"><span data-stu-id="76773-156">Images</span></span>
- <span data-ttu-id="76773-157">画像に関するメタデータ (サイズ、タグ、製品 SKU、説明)</span><span class="sxs-lookup"><span data-stu-id="76773-157">Any metadata about the images (size, tags, product SKUs, description)</span></span>
- <span data-ttu-id="76773-158">機械学習モデルによって生成されたデータ (画像あたり 4096 要素の数値ベクトルなど)</span><span class="sxs-lookup"><span data-stu-id="76773-158">Data generated by the machine learning model (for instance a 4096-element numerical vector  per image)</span></span>

<span data-ttu-id="76773-159">さまざまなソースから画像を取得したり、最適なパフォーマンスを確保するために複数の機械学習モデルを使用したりすると、データの構造が変わります。</span><span class="sxs-lookup"><span data-stu-id="76773-159">As you retrieve images from different sources or use several machine learning models for optimal performance, the structure of the data will change.</span></span> <span data-ttu-id="76773-160">そのため、半構造化データを処理することができ、固定スキーマのないテクノロジまたは組み合わせを選択することが重要です。</span><span class="sxs-lookup"><span data-stu-id="76773-160">It is therefore important to choose a technology or combination that can deal with semi-structured data and no fixed schema.</span></span>

<span data-ttu-id="76773-161">また、最小数の有用なデータ ポイント (画像識別子またはキー、製品 SKU、説明、タグ フィールドなど) が必要になる場合もあります。</span><span class="sxs-lookup"><span data-stu-id="76773-161">You may also want to require a minimum number of useful data points (e.g. an image identifier or key, a product sku, a description, a tag field).</span></span>

<span data-ttu-id="76773-162">[Azure CosmosDB](https://azure.microsoft.com/services/cosmos-db/?WT.mc_id=vsearchgio-article-gmarchet) は、その上に構築されたアプリケーションに必要な柔軟性とさまざまなアクセス メカニズムを提供します (これは、カタログ検索に役立ちます)。</span><span class="sxs-lookup"><span data-stu-id="76773-162">[Azure CosmosDB](https://azure.microsoft.com/services/cosmos-db/?WT.mc_id=vsearchgio-article-gmarchet) offers the required flexibility and a variety of access mechanisms for applications built on top of it (which will help with your catalog search).</span></span> <span data-ttu-id="76773-163">ただし、最適な価格/パフォーマンス比を実現するには注意が必要です。</span><span class="sxs-lookup"><span data-stu-id="76773-163">However, one has to be careful to drive the best price/performance.</span></span> <span data-ttu-id="76773-164">CosmosDB ではドキュメントの添付ファイルを保存できますが、アカウントごとに上限があり、コストのかかる問題になる可能性があります。</span><span class="sxs-lookup"><span data-stu-id="76773-164">CosmosDB allows document attachments to be stored, but there is a total limit per account and it may be a costly proposition.</span></span> <span data-ttu-id="76773-165">実際の画像ファイルは BLOB に保存し、それらへのリンクをデータベースに挿入するのが一般的です。</span><span class="sxs-lookup"><span data-stu-id="76773-165">It is common practice to store the actual image files in blobs and insert a link to them in the database.</span></span> <span data-ttu-id="76773-166">CosmosDB の場合、これは、その画像に関連付けられたカタログ プロパティ (SKU、タグなど) を含むドキュメントと、(Azure Blob Storage や OneDrive などにある) 画像ファイルの URL を含む添付ファイルを作成することを意味します。</span><span class="sxs-lookup"><span data-stu-id="76773-166">In the case of CosmosDB this implies creating a document that contains the catalog properties associated to that image (sku, tag etc.) and an attachment that contains the URL of the image file (e.g. on Azure blob storage, OneDrive etc).</span></span>

 ![](./assets/visual-search-use-case-overview/cosmosdb-data-model.png)

<span data-ttu-id="76773-167">*図 3: CosmosDB 階層型リソース モデル*</span><span class="sxs-lookup"><span data-stu-id="76773-167">*Figure 3: CosmosDB Hierarchical Resource Model*</span></span>

<span data-ttu-id="76773-168">Cosmos DB のグローバル配布を利用する場合、ドキュメントと添付ファイルはレプリケートされますが、リンクされたファイルはレプリケートされません。</span><span class="sxs-lookup"><span data-stu-id="76773-168">If you plan to take advantage of the global distribution of Cosmos DB, note that it will replicate the documents and attachments, but not the linked files.</span></span> <span data-ttu-id="76773-169">これらのファイルには、コンテンツ配信ネットワークを使用することを検討してください。</span><span class="sxs-lookup"><span data-stu-id="76773-169">You may want to consider a content distribution network for those.</span></span>

<span data-ttu-id="76773-170">適用可能な他のテクノロジとして、Azure SQL Database (固定スキーマを許容できる場合) と BLOB の組み合わせ、または Azure Table と BLOB の組み合わせ (安価で高速の保存と取得を実現する場合) があります。</span><span class="sxs-lookup"><span data-stu-id="76773-170">Other applicable technologies are a combination of Azure SQL Database (if fixed schema is acceptable) and blobs, or even Azure Tables and blobs for inexpensive and fast storage and retrieval.</span></span>

### <a name="feature-extraction-amp-encoding"></a><span data-ttu-id="76773-171">特徴抽出とエンコード</span><span class="sxs-lookup"><span data-stu-id="76773-171">Feature Extraction &amp; Encoding</span></span>

<span data-ttu-id="76773-172">エンコード プロセスでは、データベース内の画像から顕著な特徴を抽出し、数千の成分を持つことができる疎な "特徴" ベクトル (多数のゼロを持つベクトル) に各特徴をマップします。</span><span class="sxs-lookup"><span data-stu-id="76773-172">The encoding process extracts salient features from pictures in the database and maps each of them to a sparse "feature" vector (a vector with many zeros) that can have thousands of components.</span></span> <span data-ttu-id="76773-173">このベクトルは、画像を特徴付けるエッジや形状などの特徴の数値表現であり、コードに似ています。</span><span class="sxs-lookup"><span data-stu-id="76773-173">This vector is a numerical representation of the features (e.g. edges, shapes) that characterize the picture – akin to a code.</span></span>

<span data-ttu-id="76773-174">通常、特徴抽出手法では "_転移学習メカニズム_" を使用します。</span><span class="sxs-lookup"><span data-stu-id="76773-174">Feature extraction techniques typically use _transfer learning mechanisms_.</span></span> <span data-ttu-id="76773-175">これは、事前トレーニングされたニューラル ネットワークを選択し、それを使って各画像を実行して、生成された特徴ベクトルを画像データベースに保存するときに発生します。</span><span class="sxs-lookup"><span data-stu-id="76773-175">This occurs when you select a pre-trained neural network, run each image through it and store the feature vector  produced back in your image database.</span></span> <span data-ttu-id="76773-176">このようにして、誰かがトレーニングしたネットワークから学習を "転移" します。</span><span class="sxs-lookup"><span data-stu-id="76773-176">In that way, you "transfer" the learning from whoever trained the network.</span></span> <span data-ttu-id="76773-177">Microsoft は、事前トレーニングされたネットワークをいくつか開発し、公開しています。これらのネットワークは、画像認識タスク ([ResNet50](https://www.kaggle.com/keras/resnet50) など) に広く使用されています。</span><span class="sxs-lookup"><span data-stu-id="76773-177">Microsoft has developed and published several pre-trained networks that have been widely used for image recognition tasks, such as [ResNet50](https://www.kaggle.com/keras/resnet50).</span></span>

<span data-ttu-id="76773-178">ニューラル ネットワークによって特徴ベクトルの長さと疎の度合いが異なるため、メモリとストレージの要件が異なります。</span><span class="sxs-lookup"><span data-stu-id="76773-178">Depending on the neural network, the feature vector will be more or less long and sparse, hence the memory and storage requirements will vary.</span></span>

<span data-ttu-id="76773-179">また、ネットワークによって適用できるカテゴリがそれぞれ異なる場合があるため、画像検索の実装では、実際にはさまざまなサイズの特徴ベクトルが生成されます。</span><span class="sxs-lookup"><span data-stu-id="76773-179">Also, you may find that different networks are applicable to different categories, hence an implementation of visual search may actually generate feature vectors of varying size.</span></span>

<span data-ttu-id="76773-180">事前トレーニングされたニューラル ネットワークは比較的使いやすいですが、独自の画像カタログでトレーニングされたカスタム モデルほど効率的ではない可能性があります。</span><span class="sxs-lookup"><span data-stu-id="76773-180">Pre-trained neural networks are relatively easy to use but may not be as efficient a custom model trained on your image catalog.</span></span> <span data-ttu-id="76773-181">これらの事前トレーニングされたネットワークは、通常、特定の画像コレクションの検索ではなく、ベンチマーク データセットの分類用に設計されています。</span><span class="sxs-lookup"><span data-stu-id="76773-181">Those pre-trained networks are typically designed for classification of benchmark datasets rather than search on your specific collection of images.</span></span>

<span data-ttu-id="76773-182">カテゴリの予測と密 (より小さい、疎ではない) ベクトルの両方を生成するようにそれらを修正し、再トレーニングすることができます。これは、検索空間を制限し、メモリとストレージの要件を削減するのに非常に役立ちます。</span><span class="sxs-lookup"><span data-stu-id="76773-182">You may want to modify and retrain them so they produce both a category prediction and a dense (i.e. smaller, not sparse) vector, which will be very useful to restrict the search space, reduce memory and storage requirements.</span></span> <span data-ttu-id="76773-183">バイナリ ベクトルを使用できます。多くの場合、これは "[セマンティック ハッシュ](https://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf)" (ドキュメントのエンコードと取得の手法から派生した用語) と呼ばれます。</span><span class="sxs-lookup"><span data-stu-id="76773-183">Binary vectors can be used and are often referred to as " [semantic hash](https://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf)" – a term derived from document encoding and retrieval techniques.</span></span> <span data-ttu-id="76773-184">バイナリ表現により、後続の計算が簡素化されます。</span><span class="sxs-lookup"><span data-stu-id="76773-184">The binary representation simplifies further calculations.</span></span>

 ![](./assets/visual-search-use-case-overview/resnet-modifications.png)

<span data-ttu-id="76773-185">*図 4: 画像検索のための ResNet の変更 - F. Yang 他 (2017 年)*</span><span class="sxs-lookup"><span data-stu-id="76773-185">*Figure 4: Modifications to ResNet for Visual Search – F. Yang et al., 2017*</span></span>

<span data-ttu-id="76773-186">事前トレーニングされたモデルを選択するか、独自のモデルを開発するかに関係なく、モデル自体の特徴付けやトレーニングを実行する場所を決定する必要があります。</span><span class="sxs-lookup"><span data-stu-id="76773-186">Whether you choose pre-trained models or to develop your own, you will still need to decide where to run the featurization and/or training of the model itself.</span></span>

<span data-ttu-id="76773-187">Azure には、複数のオプション (VM、Azure Batch、[Batch AI](https://azure.microsoft.com/services/batch-ai/?WT.mc_id=vsearchgio-article-gmarchet)、Databricks クラスター) が用意されています。</span><span class="sxs-lookup"><span data-stu-id="76773-187">Azure offers several options: VMs, Azure Batch, [Batch AI](https://azure.microsoft.com/services/batch-ai/?WT.mc_id=vsearchgio-article-gmarchet), Databricks clusters.</span></span> <span data-ttu-id="76773-188">ただし、どの場合も、GPU を使用することによって、最適な価格/パフォーマンス比が得られます。</span><span class="sxs-lookup"><span data-stu-id="76773-188">In all cases, however, the best price/performance is given by the use of GPUs.</span></span>

<span data-ttu-id="76773-189">最近、Microsoft はわずかな GPU コストで高速計算を実現する FPGA の提供も発表しました ([Project Brainwave](https://www.microsoft.com/research/blog/microsoft-unveils-project-brainwave/?WT.mc_id=vsearchgio-article-gmarchet))。</span><span class="sxs-lookup"><span data-stu-id="76773-189">Microsoft has also recently announced the availability of FPGAs for fast computation at a fraction of the GPU cost (project [Brainwave](https://www.microsoft.com/research/blog/microsoft-unveils-project-brainwave/?WT.mc_id=vsearchgio-article-gmarchet)).</span></span> <span data-ttu-id="76773-190">ただし、このドキュメントの執筆時点では、このオファリングは特定のネットワーク アーキテクチャに限定されているので、パフォーマンスを綿密に評価する必要があります。</span><span class="sxs-lookup"><span data-stu-id="76773-190">However, at the time of writing, this offering is limited to certain network architectures, so you will need to evaluate their performance closely.</span></span>

### <a name="similarity-measure-or-distance"></a><span data-ttu-id="76773-191">類似性測度または距離</span><span class="sxs-lookup"><span data-stu-id="76773-191">Similarity Measure or Distance</span></span>

<span data-ttu-id="76773-192">画像を特徴ベクトル空間内で表現する場合、類似性の検出は、そのような空間内にあるポイント間の距離測度の定義の問題になります。</span><span class="sxs-lookup"><span data-stu-id="76773-192">When the images are represented in the feature vector space, finding similarities becomes a question of defining a distance measure between points in such space.</span></span> <span data-ttu-id="76773-193">距離が定義されたら、類似画像のクラスターを計算したり、類似性マトリックスを定義したりできます。</span><span class="sxs-lookup"><span data-stu-id="76773-193">Once a distance is defined, you can compute clusters of similar images and/or define similarity matrices.</span></span> <span data-ttu-id="76773-194">選択した距離メトリックによって、結果が異なる場合があります。</span><span class="sxs-lookup"><span data-stu-id="76773-194">Depending on the distance metric selected, the results may vary.</span></span> <span data-ttu-id="76773-195">理解しやすいものとして、実数ベクトルに対する最も一般的なユークリッド距離測度があります。これは距離の大きさを取得します。</span><span class="sxs-lookup"><span data-stu-id="76773-195">The most common Euclidean distance measure over real-number vectors, for instance, is easy to understand: it captures the magnitude of the distance.</span></span> <span data-ttu-id="76773-196">ただし、計算の観点から言うと、やや非効率的です。</span><span class="sxs-lookup"><span data-stu-id="76773-196">However, it is rather inefficient in terms of computation.</span></span>

<span data-ttu-id="76773-197">[コサイン](https://en.wikipedia.org/wiki/Cosine_similarity)距離は、その大きさではなく、ベクトルの向きを取得するためによく使用されます。</span><span class="sxs-lookup"><span data-stu-id="76773-197">[Cosine](https://en.wikipedia.org/wiki/Cosine_similarity) distance is often used to capture the orientation of the vector, rather than its magnitude.</span></span>

<span data-ttu-id="76773-198">バイナリ表現に対する[ハミング](https://en.wikipedia.org/wiki/Hamming_distance)距離などの代替手段では、ある程度の正確さが損なわれる代わりに、効率と速度が得られます。</span><span class="sxs-lookup"><span data-stu-id="76773-198">Alternatives such as [Hamming](https://en.wikipedia.org/wiki/Hamming_distance) distance over binary representations trade some accuracy for efficiency and speed.</span></span>

<span data-ttu-id="76773-199">ベクトルのサイズと距離測度の組み合わせによって、検索の計算負荷とメモリ負荷がどのくらいになるかが決まります。</span><span class="sxs-lookup"><span data-stu-id="76773-199">The combination of vector size and distance measure will determine how computationally intensive and memory intensive the search will be.</span></span>

### <a name="search-amp-ranking"></a><span data-ttu-id="76773-200">検索とランク付け</span><span class="sxs-lookup"><span data-stu-id="76773-200">Search &amp; Ranking</span></span>

<span data-ttu-id="76773-201">類似性が定義されたら、入力として渡されたものに最も近い N 個の項目を取得し、識別子のリストを返す効率的な方法を考案する必要があります。</span><span class="sxs-lookup"><span data-stu-id="76773-201">Once similarity is defined, we need to devise an efficient method to retrieve the closest N items to the one passed as input, then return a list of identifiers.</span></span> <span data-ttu-id="76773-202">これは "画像ランク付け" とも呼ばれます。</span><span class="sxs-lookup"><span data-stu-id="76773-202">This is also known as "image ranking".</span></span> <span data-ttu-id="76773-203">大規模なデータセットでは、すべての距離の計算に膨大な時間がかかるため、近似最近傍アルゴリズムを使用します。</span><span class="sxs-lookup"><span data-stu-id="76773-203">On a large data set, the time to compute every distance is prohibitive, so we use approximate nearest-neighbor algorithms.</span></span> <span data-ttu-id="76773-204">これらのアルゴリズムのオープン ソース ライブラリがいくつか存在するので、ゼロからコーディングする必要はありません。</span><span class="sxs-lookup"><span data-stu-id="76773-204">Several open source libraries exist for those, so you won't have to code them from scratch.</span></span>

<span data-ttu-id="76773-205">最後に、メモリと計算の要件によって、トレーニングされたモデルの展開テクノロジと高可用性が決まります。</span><span class="sxs-lookup"><span data-stu-id="76773-205">Finally, memory and computation requirements will determine the choice of deployment technology for the trained model, as well high availability.</span></span> <span data-ttu-id="76773-206">通常、検索空間はパーティション分割され、ランク付けアルゴリズムの複数のインスタンスが並列実行されます。</span><span class="sxs-lookup"><span data-stu-id="76773-206">Typically, the search space will be partitioned, and several instances of the ranking algorithm will run in parallel.</span></span> <span data-ttu-id="76773-207">スケーラビリティと可用性を実現する 1 つのオプションとして、[Azure Kubernetes](https://azure.microsoft.com/services/container-service/kubernetes/?WT.mc_id=vsearchgio-article-gmarchet) クラスターがあります。</span><span class="sxs-lookup"><span data-stu-id="76773-207">One option that allows for scalability and availability is [Azure Kubernetes](https://azure.microsoft.com/services/container-service/kubernetes/?WT.mc_id=vsearchgio-article-gmarchet) clusters.</span></span> <span data-ttu-id="76773-208">その場合、ランク付けモデルを複数のコンテナー (各検索空間のパーティションを処理) および複数のノード (高可用性の確保) に展開することをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="76773-208">In that case it is advisable to deploy the ranking model across several containers (handling a partition of the search space each) and several nodes (for high availability).</span></span>

## <a name="next-steps"></a><span data-ttu-id="76773-209">次のステップ</span><span class="sxs-lookup"><span data-stu-id="76773-209">Next steps</span></span>

<span data-ttu-id="76773-210">画像検索の実装は複雑である必要はありません。</span><span class="sxs-lookup"><span data-stu-id="76773-210">Implementing visual search need not be complex.</span></span> <span data-ttu-id="76773-211">Bing を使用することも、Azure サービスを使用して独自に構築することもでき、Microsoft の AI 研究と AI ツールからメリットが得られます。</span><span class="sxs-lookup"><span data-stu-id="76773-211">You can use Bing or build your own with Azure services, while benefiting from Microsoft's AI research and tools.</span></span>

### <a name="trial"></a><span data-ttu-id="76773-212">試用版</span><span class="sxs-lookup"><span data-stu-id="76773-212">Trial</span></span>

- <span data-ttu-id="76773-213">[Visual Search API テスト コンソール](https://dev.cognitive.microsoft.com/docs/services/878c38e705b84442845e22c7bff8c9ac)をお試しください。</span><span class="sxs-lookup"><span data-stu-id="76773-213">Try out the [Visual Search API Testing Console](https://dev.cognitive.microsoft.com/docs/services/878c38e705b84442845e22c7bff8c9ac)</span></span>

### <a name="develop"></a><span data-ttu-id="76773-214">開発</span><span class="sxs-lookup"><span data-stu-id="76773-214">Develop</span></span>

- <span data-ttu-id="76773-215">カスタマイズされたサービスの作成を開始するには、[Bing Visual Search API の概要](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/overview/?WT.mc_id=vsearchgio-article-gmarchet)をご覧ください。</span><span class="sxs-lookup"><span data-stu-id="76773-215">To begin creating a customized service, see [Bing Visual Search API Overview](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/overview/?WT.mc_id=vsearchgio-article-gmarchet)</span></span>
- <span data-ttu-id="76773-216">最初の要求を作成するには、クイック スタート ([C#](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/csharp) | [Java](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/java) | [node.js](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/nodejs) | [Python](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/python)) をご覧ください。</span><span class="sxs-lookup"><span data-stu-id="76773-216">To create your first request, see the quickstarts: [C#](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/csharp) | [Java](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/java) | [node.js](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/nodejs) | [Python](https://docs.microsoft.com/azure/cognitive-services/bing-visual-search/quickstarts/python)</span></span>
- <span data-ttu-id="76773-217">[Visual Search API リファレンス](https://aka.ms/bingvisualsearchreferencedoc)を理解します。</span><span class="sxs-lookup"><span data-stu-id="76773-217">Familiarize yourself with the [Visual Search API Reference](https://aka.ms/bingvisualsearchreferencedoc).</span></span>

### <a name="background"></a><span data-ttu-id="76773-218">バックグラウンド</span><span class="sxs-lookup"><span data-stu-id="76773-218">Background</span></span>

- <span data-ttu-id="76773-219">[ディープ ラーニングによる画像のセグメント化](https://www.microsoft.com/developerblog/2018/04/18/deep-learning-image-segmentation-for-ecommerce-catalogue-visual-search/?WT.mc_id=vsearchgio-article-gmarchet): 画像をバックグラウンドから分離するプロセスについて説明する Microsoft の論文</span><span class="sxs-lookup"><span data-stu-id="76773-219">[Deep Learning Image Segmentation](https://www.microsoft.com/developerblog/2018/04/18/deep-learning-image-segmentation-for-ecommerce-catalogue-visual-search/?WT.mc_id=vsearchgio-article-gmarchet): Microsoft paper describes the process of separating images from backgrounds</span></span>
- <span data-ttu-id="76773-220">[Visual Search at eBay (eBay での画像検索)](https://arxiv.org/abs/1706.03154): コーネル大学による研究</span><span class="sxs-lookup"><span data-stu-id="76773-220">[Visual Search at Ebay](https://arxiv.org/abs/1706.03154): Cornell University research</span></span>
- <span data-ttu-id="76773-221">[Visual Discovery at Pinterest (Pinterest での画像検索)](https://arxiv.org/abs/1702.04680): コーネル大学による研究</span><span class="sxs-lookup"><span data-stu-id="76773-221">[Visual Discovery at Pinterest](https://arxiv.org/abs/1702.04680) Cornell University research</span></span>
- <span data-ttu-id="76773-222">[Semantic Hashing (セマンティック ハッシュ)](https://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf): トロント大学による研究</span><span class="sxs-lookup"><span data-stu-id="76773-222">[Semantic Hashing](https://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf) University of Toronto research</span></span>

<span data-ttu-id="76773-223">_この記事は、Giovanni Marchetti と Mariya Zorotovich によって作成されました。_</span><span class="sxs-lookup"><span data-stu-id="76773-223">_This article was authored by Giovanni Marchetti and Mariya Zorotovich._</span></span>